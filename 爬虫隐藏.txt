一、模拟真实浏览器访问网页
Request Headers 服务端判断是否是人类的访问
例如：
User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36
User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36

修改header：(字典)
	1、通过Request的headers参数修改
	2、通过Request.add_header()方法
例如：
header = {'User-Agent':'...'}
page = request.Request(url, headers = header)
page_info = request.urlopen(page).read().decode('utf-8')
或
page.add_header(header[0])

二、延迟访问时间(效率大大降低)
在循环内部加Time.sleep(5)

三、使用代理
步骤：
1、参数是一个字典{'类型':'代理IP:端口号'}
proxy_support = urllib.request.ProxyHandler({})
2、定制、创建一个opener
opener = urllib.request.build_opener(proxy_support)
3_1、安装opener (将替换系统默认opener，以后调用的都是定制的opener)
urllib.request.install_opener(opener)
3_2、调用opener （跳过3_1，则临时使用定制的opener）
opener.open(url)

例如：
import urllib.request
import random

url = 'http://www.whatismyip.com.tw'
# url = 'http://www.ip138.com/'
# url = 'http://www.ip.cn/'

iplist = ['61.172.249.96:80','58.222.254.11:3128','61.185.219.126:3128','218.247.161.37:80']

proxy_support = urllib.request.ProxyHandler({'http':random.choice(iplist)})
opener = urllib.request.build_opener(proxy_support)
urllib.request.install_opener(opener)
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}
page = urllib.request.Request(url, headers = headers)
html = urllib.request.urlopen(page).read().decode('utf-8')
#html = opener.open(page).read().decode('utf-8')

print(html)
